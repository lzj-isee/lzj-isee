---
title: 构建文搜图评测数据集
description: 本文讲述了如何构建一个能同时计算Precision和Recall的 Text2Image Benchmark
date: 2023-11-04 21:20:00
tags:
- LLM
- CLIP
- 评测数据集
---

# 文搜图与CLIP
`CLIP`模型是OpenAI在2021年的一个多模态工作，讨论一种新型的（在当时看来）图片特征和文本特征对齐的方法，通过使用大量从互联网爬取的图文数据，`CLIP`在许多任务中展现了优秀性能。
简单来说，一个典型的`CLIP`结构包含一个图像编码器和一个文本编码器，这两个编码器分别单独将图像数据和文本数据映射成向量，在分别经过两个线性映射以及normalization后，图像特征向量和文本特征向量被映射为相同维度（`CLIP`模型最终的特征向量典型为512维）。
`CLIP`模型的训练数据是大量的图文匹配对，其中图片和文本均是互联网爬取的数据（所以可以想象这些训练数据的“质量”并不会很高😒），一般来说文本数据是对图片内容的描述，也就是说这里的文本数据在概念上对应了图片的Caption。不过由于互联网爬取的数据一般都比较脏，所以很多文本数据并不完全对应了图片的内容描述，有些看起来是广告词，有些看起来是文章的标题。
`CLIP`模型的训练loss形式上比较简单，在一个batch中，分别采样$N$个图文匹配对，然后计算一个$N\times N$的矩阵，其中第$(i,j)$位置的元素是第$i$张图片的特征向量和第$j$个文本的特征向量的内积（对角线上的元素就是图片文本匹配对的内积结果）。
然后对这个$N\times N$的矩阵分别按行和列计算SoftMax得到两个统计了图文匹配概率的矩阵，其中的一个矩阵对每张图片计算与所有文本之间的匹配概率，另一个矩阵对每个文本计算与所有图片之间的匹配概率，最后两个矩阵分别求CrossEntropy取平均。一般来说这个$N$很大，基本是10K以上。

> 上述的是CLIP原论文的训练方式，后续的工作在训练方案上会有一些改进。

在形式上，`CLIP`在每个batch中针对每个图片和每个文本训练了一个10K量级的分类任务，其中标签就是互联网爬取的数据集定义的图片文本对匹配关系，即每张图片有唯一一个对应的文本，每个文本有唯一一个对应的图片。
所以一个很自然的想法是，可以用`CLIP`来做图像和文本搜索，而且既可以做到文搜图也可以做到图搜文。具体来说，假设需要实现文搜图，那么可以使用`CLIP`的图像编码器，将待处理的图像数据集全部转化为向量，存储在向量数据库中（个人demo体验的话可以试试`Faiss`，挺好用的）。在接受到一个检索问询Query后，在线地使用`CLIP`的文本编码器将Query也转化为向量，然后调用向量数据库的功能，找到和这个Query向量最相似（内积得分最高）的TopK个图片向量和对应的图片，这就完成了图片检索的任务。

> 中文CLIP可以参考项目: [ChineseCLIP](https://github.com/OFA-Sys/Chinese-CLIP)

# 评测数据集
在构建一个文搜图系统之前，还需要评测当前系统的性能。目前来说，有很多公开的学术数据集可用于评测中文`CLIP`的Recall性能：

- [`MUGE`数据集](https://tianchi.aliyun.com/dataset/107332)：这里特指的是`MUGE`数据集的图片检索任务，该任务要求模型根据自然语言形式的检索query，从给定的商品图片池中检索出相关图片，衡量模型多模态理解与匹配的能力。测试集包含大约5k的query和3w的图片候选池，其中query的风格类似于电商场景的搜索短语。
- [`Flickr-30K-CN`数据集](https://github.com/li-xirong/cross-lingual-cap)：`Flickr-30K`的中文版本，测试集包含大约5k的query和1k的图片候选池，其中query的风格大多是一个描述图片内容的句子(caption)。
- [`COCO-CN`数据集](https://github.com/li-xirong/coco-cn)：基于`MS-COCO`构建，测试集有大约22218个人工构建的caption，以及对应的1000张图片，不过该数据集需要申请获得，非公开。
- [`Noah-Wukong`数据集](https://wukong-dataset.github.io/wukong-dataset/)：华为诺亚开源的数据集，其中测试集部分包含33365张图片以及对应的caption（query），不过我从提供的URL只能下载23106张图片。数据集应该是从百度爬取的，其中的caption部分并非人工标注的，所以风格上看起来比较凌乱。
- [`TaiSu`数据集（这个只有训练部分，不过也列在这里吧）](https://github.com/ksOAn6g5/TaiSu)：一个图文一对一的训练数据集。
- [`Zero`数据集](https://zero.so.com/)：360公司提供的数据集，其中测试部分分成了`ICM`, `IQM`, `ICR`, `IQR`四个子任务。分别是，image和caption的匹配任务（即判断image和caption之间是0还是1的匹配关系），image和query的匹配关系，image和query的检索关系（一对一的关系，没有一对多），image和query的检索关系。算是挺全面的了，其中caption是图片的内容描述（不过不是人工打标的，应该类似百度那种），query是真正用户的搜索query（至少数据集的发布方是这么宣称的）。

上面这些评测数据集的问题在于：

- 只提供了图文一对一的关系，因此只能用来算Recall，不能算Precision。
- 文本部分（除了`COCO-CN`）是互联网爬取的图文数据，比较脏，有些看起来是广告词，有些看起来是文章的标题，实在不能算是通常意义上的检索query。
- 这些图片要么来自于互联网，要么是特定领域（ `MUGE`是电商数据集，`COCO-CN`源自detection），无法针对某一需求domain评测。
- 这些数据集的训练集部分很多都被clip train过了，在同一个domain，所以指标上会比较好看，但是很难体现出实际使用中的效果。

最近业务上需要构建文搜图评测数据集，评测文搜图系统的性能。

# 基于LLM构建Recall测试集
本节考虑的问题是，如果需要在某个特定domain，比如就是一般人在日常生活中可能拍摄的一些照片（普通用户的个人相册场景）这种，如何构建对应的评测数据集。

首先需要明确的前提是，上述的场景设定假设是只能够获得一些照片数据以及这些照片的元信息，比如照片拍摄的时间和地点，但是不包含这些照片在文字域的补充信息，比如标签、主题或是照片中出现的物体和人物等等。
在这种情况下，我认为构建“检索准确度指标”（比如Precision5、Precision10）不说是毫无办法，至少也是比较困难的。因为在计算Precision时，需要为每一个问询Query指定一个图片的GroundTruth集合，这个集合的数量一般大于1（否则就只能计算Precision1了，那和Recall1是一样的）。
那么也就是说，我们需要制定一个规则，用来分辨哪些图片可以被归类为同一类，即属于同一个GroundTruth集合，绑定同一个问询Query。

这个给图片分类，或者说打标签的规则，是很难制定的，因为图片内容的变化可能会比较连续，不好设定一个阈值，区分两张图片分别属于两个不同的问询Query。
举个例子，假设一家人去西湖旅游，拍摄了很多照片，这些照面的内容假设是一家人在西湖边的合照（或独照），背景是西湖的常见景物。
那么现在考虑存在这么一张图片作为pivot，照片中出现了3个人，背景有湖水、柳树和雷峰塔。
进一步，我们可以合理假设存在很多其他内容相似的照片，比如照片是2个人或1个人，背景有或没有湖水，有或没有柳树，有或没有，雷峰塔。
那么请问，这些照片，应该被分为同一个GroundTruth，还是多个不同的GroundTruth？

一个直观的想法是，对于不同的问询Query，这些图片可能属于同一个Query，也可能属于不同的Query。比如如果Query是“西湖旅游”，那么这些照片显然应该是属于这个问询的；但是如果Query是“西湖旅游全家福”，那么至少应该要排除掉那些只出现了2个人或1个人的图片。
另一方面，别忘了我们只能使用这些照片本身的数据，没有任何关于图片的准确文字描述。
所以如果要构造一个能够评测Precision的数据集，一方面需要能够基于图片（或图片集合）生成问询Query；另一方面需要能够区分不同层次的Query，并有能力为不同的图片生成聚类，绑定同一个问询Query。
除了上述的难点外，为了评测Precision，需要保证构建评测数据集时，GroundTruth集合能找得足够全，否则测试的指标会很低，因为如果本该是GroundTruth的图片没有被纳入，无形中降低了Precision指标的上限。

关于这一点我个人的想法是，构建评测Precision的条件是，至少需要找到一个足够优秀的打标签（tag）系统。
假设存在这么一个给图片打标签的系统，能够比较准确地检测出图片中出现的内容，那么我们也许可以根据这个系统的标签GroundTruth集合，构建一颗标签树。
在最简单的情况下，假设标签集合中的所有标签都没有上下级关系，那么这个树只有两层（假设第一层的根节点是dummy），所有的标签信息都位于叶子节点；
在稍微复杂的情况下，很多模型提供的是有层级关系的标签集合，那么标签之间是有粒度关系的，比如可能某个级别的标签是“动物”，“动物”分类下有“狗”，“狗”的分类下又会有具体的品种，等等。
根据标签系统构建测试benchmark的好处是，即使这个GroundTruth标签集合比较小，那么也是不影响评测的准确性的。
考虑如下情况：假设存在一个打标签系统，所有的标签只有两个，分别是“白天”和“黑夜”，那么根据当前标签能构建出的问询Query也只有两个，那么此时评测的就是待测检索系统对“白天”和“黑夜”概念的检索效果。
即使被测图片集合中可能会存在很多内容，但是这并不影响评测，因为这些概念并不会出现在Query集合中。
此外，基于上述思想，使用打标签系统构建的评测数据集是有很好的可扩展性的，即使初版的标签树很简单，就如上述的“白天”和“黑夜”的例子，在后续依然可以持续完善标签树的复杂度，丰富测试的问询Query集合，并且无需最图片集合做出修改。

但是，构建这个一个值得信赖的标签系统是很困难的，因此我在第一次构建评测数据集时放弃了测试Precision指标。
我的思路回到了开源benchmark的做法，即为图文构建一对一的绑定关系，先搞定测试Recall指标。
然而正如上述提到的前提，我只有图片数据，因此一个自然的想法是，**能不能借助LLM的力量为图片批量生成较高质量的caption？**以LLM生成的caption作为问询Query，测试文搜图系统在特定domain的Recall性能。

## 模型选择
在当前时间节点（23年9月），我主要考虑了以下几个可用于图像描述的多模态模型：

* [`Salesforce的Blip2系列`](https://huggingface.co/Salesforce/blip2-opt-2.7b): blip系列是现在这段时间内很有代表性的多模态模型架构了，很多多模态图文大模型都是基于blip的结构。但是这个huggingface上的官方blip仓库只提供了图片英文caption功能，不适用于我当前的中文文搜图场景。
* [`Ziya-BLIP2-14B-Visual-v1`](https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1): 也是基于blip2结构的图文多模态大模型，支持中文，不过我最后没有选择该模型，原因是需要申请LLama的权重，在此期间我尝试了其他模型，效果基本符合要求，因此没有采用`Ziya`。
* [`VisualGLM-6B`](https://github.com/THUDM/VisualGLM-6B): 和知名的`ChatGLM`同源，试用后发现问答和详细图片内容描述还是很不错的。但是我的场景需要大约40字描述图片的主要内容，语言风格尽量朴实，类似`COCO-CN`数据集的caption风格。但是`VisualGLM-6B`的图片描述类似写作文，加入了很多不必要的细节描述以及个人化猜测，而且这些行为很难通过修改prompt避免（至少我自己尝试了不同prompt后发现差别不大），因此放弃使用该模型。
* [`Qwen-VL`](https://github.com/QwenLM/Qwen-VL): 这个注意要使用23年9月25日以后的版本，和第一版差别比较大。该版本和`VisualGLM-6B`相比，在我这个场景下的差异主要是`Qwen-VL`更能遵循指令要求，比较容易控制生成的图片caption的风格和详略程度，因此最后选择了该模型。


## 图片数据清洗

## Prompt模板和Generation参数