---
title: 构建文搜图评测数据集
description: 本文讲述了如何构建一个能同时计算Precision和Recall的 Text2Image Benchmark
date: 2023-11-04 21:20:00
tags:
- LLM
- CLIP
- 评测数据集
---

# 文搜图与CLIP
`CLIP`模型是OpenAI在2021年的一个多模态工作，讨论一种新型的（在当时看来）图片特征和文本特征对齐的方法，通过使用大量从互联网爬取的图文数据，`CLIP`在许多任务中展现了优秀性能。
简单来说，一个典型的`CLIP`结构包含一个图像编码器和一个文本编码器，这两个编码器分别单独将图像数据和文本数据映射成向量，在分别经过两个线性映射以及normalization后，图像特征向量和文本特征向量被映射为相同维度（`CLIP`模型最终的特征向量典型为512维）。
`CLIP`模型的训练数据是大量的图文匹配对，其中图片和文本均是互联网爬取的数据（所以可以想象这些训练数据的“质量”并不会很高😒），一般来说文本数据是对图片内容的描述，也就是说这里的文本数据在概念上对应了图片的Caption。不过由于互联网爬取的数据一般都比较脏，所以很多文本数据并不完全对应了图片的内容描述，有些看起来是广告词，有些看起来是文章的标题。
`CLIP`模型的训练loss形式上比较简单，在一个batch中，分别采样$N$个图文匹配对，然后计算一个$N\times N$的矩阵，其中第$(i,j)$位置的元素是第$i$张图片的特征向量和第$j$个文本的特征向量的内积（对角线上的元素就是图片文本匹配对的内积结果）。
然后对这个$N\times N$的矩阵分别按行和列计算SoftMax得到两个统计了图文匹配概率的矩阵，其中的一个矩阵对每张图片计算与所有文本之间的匹配概率，另一个矩阵对每个文本计算与所有图片之间的匹配概率，最后两个矩阵分别求CrossEntropy取平均。一般来说这个$N$很大，基本是10K以上。

> 上述的是CLIP原论文的训练方式，后续的工作在训练方案上会有一些改进。

在形式上，`CLIP`在每个batch中针对每个图片和每个文本训练了一个10K量级的分类任务，其中标签就是互联网爬取的数据集定义的图片文本对匹配关系，即每张图片有唯一一个对应的文本，每个文本有唯一一个对应的图片。
所以一个很自然的想法是，可以用`CLIP`来做图像和文本搜索，而且既可以做到文搜图也可以做到图搜文。具体来说，假设需要实现文搜图，那么可以使用`CLIP`的图像编码器，将待处理的图像数据集全部转化为向量，存储在向量数据库中（个人demo体验的话可以试试`Faiss`，挺好用的）。在接受到一个检索问询Query后，在线地使用`CLIP`的文本编码器将Query也转化为向量，然后调用向量数据库的功能，找到和这个Query向量最相似（内积得分最高）的TopK个图片向量和对应的图片，这就完成了图片检索的任务。

> 中文CLIP可以参考项目: [ChineseCLIP](https://github.com/OFA-Sys/Chinese-CLIP)

# 评测数据集
在构建一个文搜图系统之前，还需要评测当前系统的性能。目前来说，有很多公开的学术数据集可用于评测中文`CLIP`的Recall性能：

- `MUGE`数据集：
- `Flickr-30K`数据集：
- `COCO-CN`数据集：
- `Noah-Wukong`数据集：
- `TaiSu`数据集（这个只有训练部分，不过也列在这里吧）：
- `Zero`数据集：

上面这些评测数据集的问题在于（我个人觉得）：

- 只提供了图文一对一的关系，因此只能用来算Recall，不能算Precision。
- 文本部分（除了`COCO-CN`）是互联网爬取的图文数据，比较脏，有些看起来是广告词，有些看起来是文章的标题，实在不能算是正常人会用的检索词。
- 这些图片要么来自于互联网，要么是特定领域（ `MUGE`是电商数据集，`COCO-CN`源自detection），无法针对某一需求domain评测。
- 这些数据集的训练集部分很多都被clip train过了，在同一个domain，所以指标上会比较好看，但是很难体现出实际使用中的效果。

所以说，最近我就收到个需求，需要构建文搜图评测数据集，评测文搜图系统的性能。