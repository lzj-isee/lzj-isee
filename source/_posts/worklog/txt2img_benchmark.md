---
title: 构建文搜图评测数据集
description: 本文讲述了如何构建一个能同时计算Precision和Recall的 Text2Image Benchmark
date: 2023-11-04 21:20:00
tags:
- LLM
- CLIP
- 评测数据集
---

# 文搜图与CLIP
`CLIP`模型是OpenAI在2021年的一个多模态工作，讨论一种新型的（在当时看来）图片特征和文本特征对齐的方法，通过使用大量从互联网爬取的图文数据，`CLIP`在许多任务中展现了优秀性能。
简单来说，一个典型的`CLIP`结构包含一个图像编码器和一个文本编码器，这两个编码器分别单独将图像数据和文本数据映射成向量，在分别经过两个线性映射以及normalization后，图像特征向量和文本特征向量被映射为相同维度（`CLIP`模型最终的特征向量典型为512维）。
`CLIP`模型的训练数据是大量的图文匹配对，其中图片和文本均是互联网爬取的数据（所以可以想象这些训练数据的“质量”并不会很高😒），一般来说文本数据是对图片内容的描述，也就是说这里的文本数据在概念上对应了图片的Caption。不过由于互联网爬取的数据一般都比较脏，所以很多文本数据并不完全对应了图片的内容描述，有些看起来是广告词，有些看起来是文章的标题。
`CLIP`模型的训练loss形式上比较简单，在一个batch中，分别采样$N$个图文匹配对，然后计算一个$N\times N$的矩阵，其中第$(i,j)$位置的元素是第$i$张图片的特征向量和第$j$个文本的特征向量的内积（对角线上的元素就是图片文本匹配对的内积结果）。
然后对这个$N\times N$的矩阵分别按行和列计算SoftMax得到两个统计了图文匹配概率的矩阵，其中的一个矩阵对每张图片计算与所有文本之间的匹配概率，另一个矩阵对每个文本计算与所有图片之间的匹配概率，最后两个矩阵分别求CrossEntropy取平均。一般来说这个$N$很大，基本是10K以上。

> 上述的是CLIP原论文的训练方式，后续的工作在训练方案上会有一些改进。

在形式上，`CLIP`在每个batch中针对每个图片和每个文本训练了一个10K量级的分类任务，其中标签就是互联网爬取的数据集定义的图片文本对匹配关系，即每张图片有唯一一个对应的文本，每个文本有唯一一个对应的图片。
所以一个很自然的想法是，可以用`CLIP`来做图像和文本搜索，而且既可以做到文搜图也可以做到图搜文。具体来说，假设需要实现文搜图，那么可以使用`CLIP`的图像编码器，将待处理的图像数据集全部转化为向量，存储在向量数据库中（个人demo体验的话可以试试`Faiss`，挺好用的）。在接受到一个检索问询Query后，在线地使用`CLIP`的文本编码器将Query也转化为向量，然后调用向量数据库的功能，找到和这个Query向量最相似（内积得分最高）的TopK个图片向量和对应的图片，这就完成了图片检索的任务。

> 中文CLIP可以参考项目: [ChineseCLIP](https://github.com/OFA-Sys/Chinese-CLIP)

# 评测数据集
在构建一个文搜图系统之前，还需要评测当前系统的性能。目前来说，有很多公开的学术数据集可用于评测中文`CLIP`的Recall性能：

- [`MUGE`数据集](https://tianchi.aliyun.com/dataset/107332)：这里特指的是`MUGE`数据集的图片检索任务，该任务要求模型根据自然语言形式的检索query，从给定的商品图片池中检索出相关图片，衡量模型多模态理解与匹配的能力。测试集包含大约5k的query和3w的图片候选池，其中query的风格类似于电商场景的搜索短语。
- [`Flickr-30K-CN`数据集](https://github.com/li-xirong/cross-lingual-cap)：`Flickr-30K`的中文版本，测试集包含大约5k的query和1k的图片候选池，其中query的风格大多是一个描述图片内容的句子(caption)。
- [`COCO-CN`数据集](https://github.com/li-xirong/coco-cn)：基于`MS-COCO`构建，测试集有大约22218个人工构建的caption，以及对应的1000张图片，不过该数据集需要申请获得，非公开。
- [`Noah-Wukong`数据集](https://wukong-dataset.github.io/wukong-dataset/)：华为诺亚开源的数据集，其中测试集部分包含33365张图片以及对应的caption（query），不过我从提供的URL只能下载23106张图片。数据集应该是从百度爬取的，其中的caption部分并非人工标注的，所以风格上看起来比较凌乱。
- [`TaiSu`数据集（这个只有训练部分，不过也列在这里吧）](https://github.com/ksOAn6g5/TaiSu)：一个图文一对一的训练数据集。
- [`Zero`数据集](https://zero.so.com/)：360公司提供的数据集，其中测试部分分成了`ICM`, `IQM`, `ICR`, `IQR`四个子任务。分别是，image和caption的匹配任务（即判断image和caption之间是0还是1的匹配关系），image和query的匹配关系，image和query的检索关系（一对一的关系，没有一对多），image和query的检索关系。算是挺全面的了，其中caption是图片的内容描述（不过不是人工打标的，应该类似百度那种），query是真正用户的搜索query（至少数据集的发布方是这么宣称的）。

上面这些评测数据集的问题在于（我个人觉得）：

- 只提供了图文一对一的关系，因此只能用来算Recall，不能算Precision。
- 文本部分（除了`COCO-CN`）是互联网爬取的图文数据，比较脏，有些看起来是广告词，有些看起来是文章的标题，实在不能算是通常意义上的检索query。
- 这些图片要么来自于互联网，要么是特定领域（ `MUGE`是电商数据集，`COCO-CN`源自detection），无法针对某一需求domain评测。
- 这些数据集的训练集部分很多都被clip train过了，在同一个domain，所以指标上会比较好看，但是很难体现出实际使用中的效果。

最近业务上需要构建文搜图评测数据集，评测文搜图系统的性能。

# 基于LLM构建Recall数据集
本节考虑的问题是，如果需要在某个特定domain，比如就是一般人在日常生活中可能拍摄的一些照片这种，如何构建评测数据集。
